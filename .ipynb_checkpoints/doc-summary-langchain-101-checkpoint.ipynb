{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ad7fb7-4736-4e3a-a066-03725445af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992323ce-abe0-4bc1-afb8-833ff71dfd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_version = \"2024-07-18\"\n",
    "api_version = \"2024-02-15-preview\"\n",
    "azure_endpoint=\"https://nshan-m3oer8jc-eastus.openai.azure.com/\"\n",
    "azureClient = AzureOpenAI(api_version=api_version, azure_endpoint=azure_endpoint)\n",
    "# azureClient = AzureOpenAI(azure_endpoint=azure_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c702b33d-6866-445a-840a-fcad9db1dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'the course has already started, can I still enroll?'\n",
    "response = azureClient.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b842078-9303-4036-af21-d3fb1e108969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You'll need to check the specific course’s enrollment policies, as these can vary widely. Some courses allow late enrollment within a certain period after the course has started, while others do not accept late enrollees at all. It’s best to contact the course administrator or the institution offering the course directly to inquire about the possibility of enrolling late. They may also have policies for catching up on missed material if late enrollment is permitted.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b04cd11-0305-4541-83bb-3782530d4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the book\n",
    "loader = PyPDFLoader(\"https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6354c75e-2c2b-4e8f-b4e8-7fc220cb449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out the open and closing parts\n",
    "pages = pages[6:1308]\n",
    "# Combine the pages, and replace the tabs with spaces\n",
    "text = ' '.join([page.page_content.replace('\\t', ' ') for page in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3dbe2c7-c93a-4253-93e7-40a1745c5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "   # Remove extra spaces\n",
    "   cleaned_text = re.sub(r' +', ' ', text)\n",
    "   # Remove non-printable characters, optionally preceded by 'David Copperfield'\n",
    "   cleaned_text = re.sub(r'[\\x00-\\x1F]', '', cleaned_text)\n",
    "   # Replace newline characters with spaces\n",
    "   cleaned_text = cleaned_text.replace('\\n', ' ')\n",
    "   # Remove spaces around hyphens\n",
    "   cleaned_text = re.sub(r'\\s*-\\s*', '', cleaned_text)\n",
    "   return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451e7e13-fdba-4043-8f64-0b94fc522663",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d402b586-d490-4f73-b0d1-ec5a8c1cbd25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<openai.lib.azure.AzureOp...bject at 0x718e012e8830>, input_type=AzureOpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<openai.lib.azure.AzureOp...bject at 0x718e012e8830>, input_type=AzureOpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_summarize_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mazureClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmap_reduce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain/chains/summarize/chain.py:167\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[0;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain/chains/summarize/chain.py:60\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[0;34m(llm, map_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, token_max, callbacks, collapse_max_retries, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_map_reduce_chain\u001b[39m(\n\u001b[1;32m     45\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     46\u001b[0m     map_prompt: BasePromptTemplate \u001b[38;5;241m=\u001b[39m map_reduce_prompt\u001b[38;5;241m.\u001b[39mPROMPT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MapReduceDocumentsChain:\n\u001b[0;32m---> 60\u001b[0m     map_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     _reduce_llm \u001b[38;5;241m=\u001b[39m reduce_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m     67\u001b[0m     reduce_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     68\u001b[0m         llm\u001b[38;5;241m=\u001b[39m_reduce_llm,\n\u001b[1;32m     69\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mcombine_prompt,\n\u001b[1;32m     70\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<openai.lib.azure.AzureOp...bject at 0x718e012e8830>, input_type=AzureOpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<openai.lib.azure.AzureOp...bject at 0x718e012e8830>, input_type=AzureOpenAI]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6e575-d882-4d1e-982a-db04bf0f5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.run(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "407592fd-07e3-4c69-8bef-30d83f0e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "446c81b4-8ff6-47be-9df8-2231bb9e4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = AzureOpenAI(\n",
    "#     model='gpt-4o'\n",
    "# )\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae1ea54b-f632-429d-a0fa-36bd67ba8a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9283"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "862433f7-0f5e-4155-a7e5-34190a028caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3921d60-bcad-4e87-bec1-990a4eaea015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "361cc0e8-d0b4-4a2e-bbdc-7893fe623939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "411af48a-3058-4009-9fe2-e59033ff0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f6c55fc-9e07-4aba-bba6-99a83a2c4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43a7b90f-8e68-4a1d-90db-3d8145494c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final consolidated summary of the main themes across the provided documents on the MLOps lifecycle is as follows:\n",
      "\n",
      "1. **MLOps and Lifecycle Management**: MLOps practices are crucial for managing the entire lifecycle of machine learning models, from development to deployment and beyond. This includes ensuring compliance, traceability, and performance through effective governance and operationalization.\n",
      "\n",
      "2. **Model Development and Experimentation**: The initial stages of machine learning focus on data preparation, feature engineering, and model prototyping. Experimentation is iterative, requiring robust tracking for reproducibility and collaboration among teams.\n",
      "\n",
      "3. **Training and Continuous Training**: Automation of training pipelines, including data preprocessing and hyperparameter tuning, is essential. Continuous training addresses the need for model retraining in response to new data or performance decay, often facilitated by CI/CD workflows.\n",
      "\n",
      "4. **Model Deployment and Serving**: Efficient deployment and serving of models in production environments are critical. This includes the use of CI/CD routines, model serving infrastructure, and no-code/low-code solutions to streamline processes and ensure low-latency, high-throughput predictions.\n",
      "\n",
      "5. **Continuous Monitoring and Performance Evaluation**: Ongoing monitoring of model performance is vital to detect anomalies, data drift, and performance degradation. This involves using ground truth labels for evaluation and retraining, ensuring models remain accurate and reliable.\n",
      "\n",
      "6. **Data and Feature Management**: Effective management of data assets and features is necessary to avoid training-serving skew and ensure consistency. Centralized repositories promote standardization, reusability, and collaboration, enhancing data governance and discoverability.\n",
      "\n",
      "7. **Model Governance and Compliance**: Ensuring models meet business, legal, and ethical standards is essential. This involves model evaluation, explainability, risk management, and adherence to regulations, maintaining accountability and trust in deployed models.\n",
      "\n",
      "8. **Collaboration and Integration**: Collaboration among data scientists, engineers, and operations teams is emphasized, along with integration with tools and technologies to automate and streamline ML workflows. Sharing knowledge and leveraging past work improve efficiency and performance.\n",
      "\n",
      "9. **Infrastructure and Automation**: Robust IT infrastructure supports ML workflows, with cloud and on-premises integration. Automation and orchestration of ML processes, from data ingestion to model deployment, are facilitated by CI/CD workflows and infrastructure-as-code tools.\n",
      "\n",
      "10. **Educational Resources and Best Practices**: Providing educational resources and best practices is important for effective MLOps implementation. Guidance on using tools and services, such as those offered by cloud platforms, supports successful machine learning operations.\n",
      "\n",
      "These themes collectively address the comprehensive management, operationalization, and optimization of machine learning workflows and assets, focusing on automation, integration, and continuous improvement to ensure responsible and effective use of ML models in production environments.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_chain.run(split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b5a29-e305-4111-99d0-9b8f2c78caa1",
   "metadata": {},
   "source": [
    "Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d8004-f684-4604-afff-ca0333091f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8fe3a-e852-449d-9820-0d95878340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Italian\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607062d-73ef-4645-bc53-13e245b95c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42e22f-bc18-48ad-b47a-0893bbb90d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd81217-9506-436a-8da3-74496569580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "\n",
    "summarize_document_chain = AnalyzeDocumentChain(\n",
    "    combine_docs_chain=chain, text_splitter=text_splitter\n",
    ")\n",
    "summarize_document_chain.run(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc33eae-2f62-4322-a557-0ef93b31599c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
